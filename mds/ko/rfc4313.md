

```text
Network Working Group                                            D. Oran
Request for Comments: 4313                           Cisco Systems, Inc.
Category: Informational                                    December 2005

                Requirements for Distributed Control of
                  Automatic Speech Recognition (ASR),
       Speaker Identification/Speaker Verification (SI/SV), and
                     Text-to-Speech (TTS) Resources
```

---
# **Status of this Memo**

이 메모는 인터넷 커뮤니티에 대한 정보를 제공합니다. 어떤 종류의 인터넷 표준도 지정하지 않습니다. 이 메모의 배포는 무제한입니다.

---
# **Copyright Notice**

저작권\(C\)인터넷학회\(2005\).

---
# **Abstract**

이 문서는 오디오 스트림의 분산 음성 처리를 제어하는 프로토콜에 대한 필요성과 요구 사항을 설명합니다. 이 문서에서 음성 처리란 구체적으로 자동 음성 인식\(ASR\), 화자 인식\(화자 식별\(SI\)과 화자 검증\(SV\) 모두 포함\) 및 텍스트 음성 변환\(TTS\)을 의미합니다. SIP 및 실시간 스트리밍 프로토콜\(RTSP\)과 같은 다른 IETF 프로토콜은 일반화된 미디어 스트림에 대한 랑데부와 제어를 다룹니다. 그러나 음성 처리에는 기존 IETF 프로토콜 중 어느 것도 다루지 않는 추가 요구 사항이 있습니다.

---
# **Table of Contents**

```text
   1. Introduction ....................................................3
      1.1. Document Conventions .......................................3
   2. SPEECHSC Framework ..............................................4
      2.1. TTS Example ................................................5
      2.2. Automatic Speech Recognition Example .......................6
      2.3. Speaker Identification example .............................6
   3. General Requirements ............................................7
      3.1. Reuse Existing Protocols ...................................7
      3.2. Maintain Existing Protocol Integrity .......................7
      3.3. Avoid Duplicating Existing Protocols .......................7
      3.4. Efficiency .................................................8
      3.5. Invocation of Services .....................................8
      3.6. Location and Load Balancing ................................8
      3.7. Multiple Services ..........................................8
      3.8. Multiple Media Sessions ....................................8
      3.9. Users with Disabilities ....................................9
      3.10. Identification of Process That Produced Media or
            Control Output ............................................9
   4. TTS Requirements ................................................9
      4.1. Requesting Text Playback ...................................9
      4.2. Text Formats ...............................................9
           4.2.1. Plain Text ..........................................9
           4.2.2. SSML ................................................9
           4.2.3. Text in Control Channel ............................10
           4.2.4. Document Type Indication ...........................10
      4.3. Control Channel ...........................................10
      4.4. Media Origination/Termination by Control Elements .........10
      4.5. Playback Controls .........................................10
      4.6. Session Parameters ........................................11
      4.7. Speech Markers ............................................11
   5. ASR Requirements ...............................................11
      5.1. Requesting Automatic Speech Recognition ...................11
      5.2. XML .......................................................11
      5.3. Grammar Requirements ......................................12
           5.3.1. Grammar Specification ..............................12
           5.3.2. Explicit Indication of Grammar Format ..............12
           5.3.3. Grammar Sharing ....................................12
      5.4. Session Parameters ........................................12
      5.5. Input Capture .............................................12
   6. Speaker Identification and Verification Requirements ...........13
      6.1. Requesting SI/SV ..........................................13
      6.2. Identifiers for SI/SV .....................................13
      6.3. State for Multiple Utterances .............................13
      6.4. Input Capture .............................................13
      6.5. SI/SV Functional Extensibility ............................13
   7. Duplexing and Parallel Operation Requirements ..................13
      7.1. Full Duplex Operation .....................................14
      7.2. Multiple Services in Parallel .............................14
      7.3. Combination of Services ...................................14
   8. Additional Considerations (Non-Normative) ......................14
   9. Security Considerations ........................................15
      9.1. SPEECHSC Protocol Security ................................15
      9.2. Client and Server Implementation and Deployment ...........16
      9.3. Use of SPEECHSC for Security Functions ....................16
   10. Acknowledgements ..............................................17
   11. References ....................................................18
      11.1. Normative References .....................................18
      11.2. Informative References ...................................18
```

---
## **1.  Introduction**

미디어 세션\(SIP \[6\]\)의 설정 및 종료, 저수준 미디어 제어\(Media Gateway Control Protocol\(MGCP\) \[7\] 및 Media Gateway Controller\(MEGACO\) \[8\]\), 미디어 기록 및 재생\(RTSP \[9\]\)을 위한 여러 IETF 프로토콜이 있습니다. 이 문서는 자동 음성 인식\(ASR\), 화자 식별 또는 검증\(SI/SV\) 및 텍스트를 오디오로 렌더링\(텍스트 음성 변환\(TTS\)이라고도 함\)하는 네트워크 요소의 제어를 지원하기 위한 하나 이상의 프로토콜에 대한 요구 사항에 중점을 둡니다. 많은 멀티미디어 애플리케이션은 자동 음성 인식\(ASR\) 및 텍스트 음성 변환\(TTS\) 처리를 분산된 네트워크 리소스로 사용할 수 있는 이점을 얻을 수 있습니다. 이 요구 사항 문서는 ASR, SI/SV 및 TTS 서버의 분산 제어에 초점을 맞춥니다.

TTS, ASR 및 SI/SV 제어에 대한 통합된 접근 방식을 통해 이점을 얻을 수 있는 광범위한 시스템이 있습니다. 여기에는 PSTN\(공중 교환 전화망\)에 대한 VoIP\(Voice over IP\) 게이트웨이, IP 전화, 미디어 서버 및 네트워크의 서버를 통해 음성 서비스를 얻는 무선 모바일 장치와 같은 환경이 포함됩니다.

현재까지 이 문제를 다루는 다수의 독점 ASR 및 TTS API와 두 개의 IETF 문서가 있습니다\[13\], \[14\]. 그러나 기존 문서에는 심각한 결함이 있습니다. 특히 기존 프로토콜의 의미론을 혼합하면서도 구현자에게 혼란을 줄 정도로 다른 프로토콜과 충분히 유사합니다.

이 문서는 오디오 스트림의 분산 음성 처리를 지원하는 프로토콜에 대한 요구 사항을 제시합니다. 단순화를 위해, 그리고 기존 프로토콜 제안과의 혼란을 없애기 위해, 이 문서는 음성 리소스의 분산 제어를 다루는 "프레임워크"에 대한 요구 사항을 제시합니다. 이 문서는 이러한 프레임워크를 "SPEECHSC", 즉 음성 서비스 제어라고 합니다.

---
### **1.1.  Document Conventions**

이 문서에서 핵심 단어 "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT", "SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY" 및 "OPTIONAL"은 RFC 2119 \[3\]에 설명된 대로 해석되어야 합니다.

---
## **2.  SPEECHSC Framework**

아래의 그림 1은 음성 처리를 위한 SPEECHSC 프레임워크를 보여줍니다.

```text
                          +-------------+
                          | Application |
                          |   Server    |\
                          +-------------+ \ SPEECHSC
            SIP, VoiceXML,  /              \
             etc.          /                \
           +------------+ /                  \    +-------------+
           |   Media    |/       SPEECHSC     \---| ASR, SI/SV, |
           | Processing |-------------------------| and/or TTS  |
       RTP |   Entity   |           RTP           |    Server   |
      =====|            |=========================|             |
           +------------+                         +-------------+

                       Figure 1: SPEECHSC Framework
```

"미디어 처리 엔티티"는 미디어를 처리하는 네트워크 요소입니다. 순수한 미디어 핸들러일 수도 있고, 연관된 SIP 사용자 에이전트, VoiceXML 브라우저 또는 다른 제어 엔티티가 있을 수도 있습니다. "ASR, SI/SV 및/또는 TTS 서버"는 백엔드 음성 처리를 수행하는 네트워크 요소입니다. 텍스트 입력\(TTS\)을 기반으로 RTP 스트림을 출력으로 생성하거나 입력\(ASR, SI/SV\)으로 RTP 스트림에 대한 응답으로 인식 결과를 반환할 수 있습니다. "애플리케이션 서버"는 미디어 처리 엔티티에 미디어 스트림에 어떤 변환을 해야 하는지 지시하는 네트워크 요소입니다. 이러한 지시는 SIP와 같은 세션 프로토콜을 통해 설정되거나 VoiceXML과 같은 클라이언트/서버 교환을 통해 제공될 수 있습니다. 이 프레임워크를 사용하면 미디어 처리 엔티티 또는 애플리케이션 서버가 SPEECHSC를 제어 프로토콜로 사용하여 ASR 또는 TTS 서버를 제어할 수 있으며, 이는 다이어그램에 SPEECHSC 프로토콜이 두 번 나타나는 이유입니다.

엔티티의 물리적 구현은 엔티티당 하나의 물리적 인스턴스 또는 엔티티의 일부 조합에 상주할 수 있습니다. 예를 들어, VoiceXML\[11\] 게이트웨이는 미디어 처리 엔티티와 동일한 플랫폼에서 ASR 및 TTS 기능을 결합할 수 있습니다. VoiceXML 게이트웨이 자체는 이 프로토콜의 범위를 벗어난다는 점에 유의하십시오. 마찬가지로 대화형 음성 응답\(IVR\) 플랫폼의 경우와 같이 애플리케이션 서버와 미디어 처리 엔티티를 결합할 수 있습니다.

미디어 처리 엔터티를 미디어 엔드포인트를 제어하는 엔터티와 미디어를 직접 처리하는 엔터티로 분해할 수도 있습니다. 이는 MGCP 또는 MEGACO를 사용하는 분해된 게이트웨이의 경우와 같습니다. 그러나 이 분해는 다시 다음과 직교합니다.

SPEECHSC의 범위. 다음 하위 섹션에서는 TTS, ASR 및 SI/SV에 대한 SPEECHSC의 여러 가지 사용 사례를 제공합니다. 이는 단지 설명을 위한 것이며 프레임워크의 범위에 대한 제한을 의미하거나 분해 또는 구성을 예에서 표시된 것으로 제한하려는 것이 아닙니다.

---
### **2.1.  TTS Example**

이 예는 텍스트 오류 메시지를 표시할 수 없는 전화기에서 사용자에게 공지 사항을 재생하기 위한 텍스트 음성 변환 서비스를 제공하기 위해 SPEECHSC를 간단히 사용하는 방법을 보여줍니다. 예제 시나리오는 아래 그림 2에 나와 있습니다. 그림에서 VoIP 게이트웨이는 그림 1의 SPEECHSC 프레임워크의 미디어 처리 엔티티와 애플리케이션 서버 역할을 모두 합니다.

```text
                                      +---------+
                                     _|   SIP   |
                                   _/ |  Server |
                +-----------+  SIP/   +---------+
                |           |  _/
    +-------+   |   VoIP    |_/
    | POTS  |___| Gateway   |   RTP   +---------+
    | Phone |   | (SIP UA)  |=========|         |
    +-------+   |           |\_       | SPEECHSC|
                +-----------+  \      |   TTS   |
                                \__   |  Server |
                             SPEECHSC |         |
                                    \_|         |
                                      +---------+

               Figure 2: Text-to-Speech Example of SPEECHSC
```

왼쪽의 일반 전화 서비스\(POTS\) 전화는 전화를 걸려고 시도합니다. SIP UA 역할을 하는 VoIP 게이트웨이는 SIP 세션을 설정하여 전화를 완료하려고 시도하지만 SIP "486 Busy Here" 응답과 같은 오류가 발생합니다. SPEECHSC가 없다면 게이트웨이는 POTS 전화에 통화 중 신호만 출력할 가능성이 큽니다. 그러나 SPEECHSC가 TTS 서버에 액세스하면 음성 오류 메시지를 제공할 수 있습니다. 따라서 VoIP 게이트웨이는 "978-555-1212로 걸려온 전화가 통화 상대방이 통화 중이어서 연결되지 않았습니다"와 같은 SIP 메시지의 정보를 사용하여 텍스트 오류 문자열을 구성합니다. 그런 다음 SPEECHSC를 사용하여 SPEECHSC 서버와 연결을 설정하고 자체와 서버 간에 RTP 스트림을 열고 오류 메시지에 대한 TTS 요청을 발행하여 POTS 전화에서 사용자에게 재생합니다.

---
### **2.2.  Automatic Speech Recognition Example**

이 예에서는 SPEECHSC 프레임워크를 사용하여 Interactive Voice Response\(IVR\) 시스템을 통해 ASR 기반 사용자 인터페이스를 제공하는 VXML 지원 미디어 처리 엔티티와 관련 애플리케이션 서버를 보여줍니다. 예제 시나리오는 아래 그림 3에 나와 있습니다. VXML 클라이언트는 "미디어 처리 엔티티"에 해당하고, IVR 애플리케이션 서버는 그림 1의 SPEECHSC 프레임워크의 "애플리케이션 서버"에 해당합니다.

```text
                                      +------------+
                                      |    IVR     |
                                     _|Application |
                               VXML_/ +------------+
                +-----------+  __/
                |           |_/       +------------+
    PSTN Trunk  |   VoIP    | SPEECHSC|            |
   =============| Gateway   |---------| SPEECHSC   |
                |(VXML voice|         |   ASR      |
                | browser)  |=========|  Server    |
                +-----------+   RTP   +------------+

              Figure 3: Automatic Speech Recognition Example
```

이 예에서 사용자는 주식 시세를 얻기 위해 서비스에 전화를 겁니다. VoIP 게이트웨이는 PSTN 통화에 응답합니다. IVR 애플리케이션은 게이트웨이에 VXML 스크립트를 공급하여 사용자 상호 작용을 구동합니다. 게이트웨이의 VXML 인터프리터는 사용자의 미디어 스트림을 SPEECHSC ASR 서버로 보내고 SPEECHSC를 사용하여 ASR 서버를 제어합니다.

예를 들어, 사용자가 IVR 프롬프트에 응답하여 주식 이름을 말하면 SPEECHSC ASR 서버는 이름을 인식하려고 시도하고 결과를 VXML 게이트웨이로 반환합니다. VXML 게이트웨이는 표준 VXML 메커니즘을 따라 IVR 애플리케이션에 인식된 결과를 알립니다. 그러면 IVR 애플리케이션이 적절한 정보 조회를 수행할 수 있습니다. 물론 답변은 텍스트 음성 변환을 사용하여 사용자에게 다시 보낼 수 있습니다. 이 예에서는 이 시나리오를 보여주지 않지만 섹션 2.1에 표시된 시나리오와 유사하게 작동합니다.

---
### **2.3.  Speaker Identification example**

이 예는 스피커 식별을 사용하여 IP 전화에 음성으로 로그인하는 것을 허용하는 방법을 보여줍니다. 예제 시나리오는 아래 그림 4에 나와 있습니다. 그림에서 IP 전화는 그림 1의 SPEECHSC 프레임워크의 "미디어 처리 엔티티"와 "애플리케이션 서버" 역할을 모두 합니다.

```text
   +-----------+         +---------+
   |           |   RTP   |         |
   |   IP      |=========| SPEECHSC|
   |  Phone    |         |   TTS   |
   |           |_________|  Server |
   |           | SPEECHSC|         |
   +-----------+         +---------+

                 Figure 4: Speaker Identification Example
```

이 예에서 사용자는 SIP 전화에 대고 말을 하여 해당 전화에 "로그인"하여 자신의 신원과 기본 설정을 사용하여 전화를 걸고 받습니다. IP 전화는 SPEECHSC 프레임워크를 사용하여 전화와 SPEECHSC SI/SV 서버 간에 RTP 스트림을 설정하고 검증을 요청합니다. SV 서버는 사용자의 신원을 검증하고 필요한 로그인 자격 증명을 포함한 결과를 SPEECHSC를 통해 전화로 반환합니다. IP 전화는 신원을 직접 사용하여 발신 통화에서 사용자를 식별하거나, 구성 서버에서 사용자의 기본 설정을 가져오거나, 인증, 권한 부여 및 회계\(AAA\) 서버에서 권한을 요청할 수 있으며, 이를 어떤 조합으로든 사용할 수 있습니다. 이 예에서는 SPEECHSC를 사용하여 보안 관련 기능을 수행하므로 섹션 9의 관련 자료를 반드시 참고하십시오.

---
## **3.  General Requirements**
---
### **3.1.  Reuse Existing Protocols**

가능한 한, SPEECHSC 프레임워크는 기존 프로토콜을 사용해야 합니다.

---
### **3.2.  Maintain Existing Protocol Integrity**

섹션 3.1의 요구 사항을 충족하기 위해 SPEECHSC 프레임워크는 기존 프로토콜의 의미를 재정의해서는 안 됩니다. 달리 말하면, 기존 프로토콜을 깨거나 이전 버전과의 호환성 문제를 일으키지 않습니다.

---
### **3.3.  Avoid Duplicating Existing Protocols**

가능한 한, SPEECHSC는 기존 프로토콜의 기능을 복제해서는 안 됩니다. 예를 들어, SIP\[12\] 및 RTSP\[9\]를 사용하는 네트워크 발표는 이미 오디오 재생을 요청하는 방법을 정의하고 있습니다. SPEECHSC의 초점은 기존 프로토콜에서 다루지 않는 새로운 기능이나 요구 사항의 제약 내에서 기존 프로토콜을 확장하는 것입니다.

섹션 3.2. 기존 프로토콜이 SPEECHSC 요구 사항을 지원하도록 우아하게 확장될 수 있는 경우, 이러한 확장은 요구 사항을 충족하기 위한 허용 가능한 대안입니다.

이에 대한 당연한 귀결로, SPEECHSC는 미디어 스트림 리디렉션이나 기능 검색 등 SPEECHSC 프로토콜에 쉽게 추가할 수 있는 기능을 수행하기 위해 별도의 프로토콜이 필요하지 않습니다. 다만 해당 프로토콜을 SPEECHSC 프레임워크에 직접 내장하는 것이 쉬운 경우는 예외입니다.

---
### **3.4.  Efficiency**

SPEECHSC 프레임워크는 효율적인 운영으로 이어지는 것으로 알려진 프로토콜 요소를 사용해야 합니다. 고려해야 할 기술은 다음과 같습니다.

- 세션 간 전송 연결 재사용
- 역방향 요청에 대한 응답 피기백
- 요청 간 상태 캐싱

---
### **3.5.  Invocation of Services**

SPEECHSC 프레임워크는 IAB Open Pluggable Edge Services\(OPES\) \[4\] 프레임워크를 준수해야 합니다. 따라서 SPEECHSC 프로토콜의 적용 가능성은 서비스를 요청하는 사용자를 대신하여 직접 작동하는 클라이언트와 서버 간에 발생하는 것으로 지정됩니다.

---
### **3.6.  Location and Load Balancing**

가능한 한, SPEECHSC 프레임워크는 서비스 위치 프로토콜\[13\]이나 DNS SRV 레코드\[14\]와 같은 서비스 위치 및 부하 분산을 지원하기 위한 기존 계획을 활용해야 합니다. 이러한 시설이 적절하지 않은 것으로 간주되는 경우, SPEECHSC 프레임워크는 추가 부하 분산 기술을 정의할 수 있습니다.

---
### **3.7.  Multiple Services**

SPEECHSC 프레임워크는 단일 미디어 스트림에서 여러 서비스가 작동할 수 있도록 허용해야 하므로 동일하거나 다른 서버가 음성 인식, 화자 식별 또는 검증 등을 병렬로 수행할 수 있습니다.

---
### **3.8.  Multiple Media Sessions**

SPEECHSC 프레임워크는 세션과 RTP 채널 간의 1:N 매핑을 허용해야 합니다. 예를 들어, 단일 세션에는 TTS에 대한 아웃바운드 RTP 채널, ASR에 대한 인바운드, SI/SV에 대한 다른 인바운드가 포함될 수 있습니다\(예: 미디어 리소스의 다른 요소에서 처리되는 경우\)

요소\). 참고: 이 모든 것은 SDP를 통해 설명될 수 있으므로 SDP를 미디어 채널 설명에 활용하면 이 요구 사항은 "무료로" 충족됩니다.

---
### **3.9.  Users with Disabilities**

SPEECHSC 프레임워크는 장애인의 중요한 요구 사항을 해결할 수 있는 충분한 기능을 갖춰야 합니다. 특히, RFC 3351\[5\]에 명시된 요구 사항 집합은 프레임워크에서 반드시 고려해야 합니다. 또한 SPEECHSC 클라이언트 및 서버 구현자는 SPEECHSC의 일부 상호 작용 모드가 장애인 사용자에게 불편하거나 단순히 부적절할 수 있음을 인식하는 것이 중요합니다. 청각 장애인은 TTS의 유용성이 제한적일 수 있습니다. 언어 장애 사용자는 ASR 또는 SI/SV 기능을 사용하지 못할 수 있습니다. 따라서 SPEECHSC를 사용하는 시스템은 대체 상호 작용 모드를 제공하거나 음성 처리 사용을 완전히 피해야 합니다.

---
### **3.10.  Identification of Process That Produced Media or Control Output**

SPEECHSC 작업의 클라이언트는 SPEECHSC 프레임워크를 통해 어떤 음성 프로세스가 출력을 생성했는지 확인할 수 있어야 합니다. 예를 들어, TTS의 음성 출력을 포함하는 RTP 스트림은 TTS 출력으로 식별 가능해야 하며, ASR의 인식된 발화는 ASR 처리에 의해 생성된 것으로 식별 가능해야 합니다.

---
## **4.  TTS Requirements**
---
### **4.1.  Requesting Text Playback**

SPEECHSC 프레임워크는 제어 프로토콜을 사용하여 미디어 처리 엔터티나 애플리케이션 서버가 TTS 서버에 RTP 스트림의 텍스트를 음성으로 재생하도록 요청할 수 있도록 해야 합니다.

---
### **4.2.  Text Formats**
---
#### **4.2.1.  Plain Text**

SPEECHSC 프레임워크는 모든 TTS 서버가 일반 텍스트를 읽을 수 있다고 가정할 수 있습니다. 일반 텍스트를 읽기 위해 프레임워크는 세션 매개변수를 통해 언어와 음성을 표시할 수 있도록 해야 합니다. 이러한 속성에 대한 보다 세부적인 제어는 \[1\]을 참조하십시오.

---
#### **4.2.2.  SSML**

SPEECHSC 프레임워크는 SSML\(Speech Synthesis Markup Language\)\[1\] <speak\> 기본을 지원해야 하며 다른 SSML 태그를 지원해야 합니다. 프레임워크는 모든 TTS 서버가 SSML을 읽을 수 있다고 가정합니다.

포맷된 텍스트. 단일 발화 내에서 다국어 출력을 포함한 SPEECHSC 프레임워크에서 TTS의 국제화는 SSML xml:lang 태그를 통해 달성됩니다.

---
#### **4.2.3.  Text in Control Channel**

SPEECHSC 프레임워크는 모든 TTS 서버가 RTP 연결을 통해 읽기 위해 SPEECHSC 연결을 통해 텍스트를 수락한다고 가정합니다. 이 프레임워크는 서버가 "값으로"\(프로토콜에 내장됨\) 또는 "참조로"\(예: 프로토콜에 내장된 URI\(Uniform Resource Identifier\)를 역참조하여\) 텍스트를 수락할 수 있다고 가정합니다.

---
#### **4.2.4.  Document Type Indication**

문서 유형은 읽을 텍스트가 인코딩되는 구문을 지정합니다. SPEECHSC 프레임워크는 서버가 다른 수단으로 콘텐츠를 추론하도록 강제하는 것과는 달리 처리할 텍스트의 문서 유형을 명시적으로 나타낼 수 있어야 합니다.

---
### **4.3.  Control Channel**

SPEECHSC 프레임워크는 세션이 단일 "호출" 또는 "대화"와 연관되도록 느슨하게 정의된 경우 세션당 클라이언트와 서버 간의 제어 채널을 설정할 수 있어야 합니다. 프로토콜은 여러 세션에 대한 장기 제어 채널을 연속적으로 유지할 수 있어야 하며, 단일 발화 처리와 같이 짧은 시간 범위를 포함하여 더 짧은 시간 범위도 가능할 수 있습니다.

---
### **4.4.  Media Origination/Termination by Control Elements**

SPEECHSC 프레임워크는 제어 요소\(애플리케이션 서버, 미디어 처리 엔터티\)가 미디어 스트림을 수락하거나 생성하도록 요구해서는 안 됩니다. 미디어 스트림은 제어되는 요소\(ASR, TTS 등\)에서 소스 및 싱크될 수 있습니다.

---
### **4.5.  Playback Controls**

SPEECHSC 프레임워크는 SPEECHSC 처리에서 스트리밍 미디어 출력의 재생을 제어하기 위한 "VCR 컨트롤"을 지원해야 하며, 다양한 기능을 갖춘 서버가 이러한 컨트롤을 수용할 수 있도록 허용해야 합니다. 이 프로토콜은 클라이언트가 사용하고자 하는 컨트롤을 명시하고, 서버가 어떤 컨트롤을 존중하는지 보고할 수 있도록 허용해야 합니다. 이러한 기능에는 다음이 포함됩니다.

- 특정 마커의 위치로 시간적으로 점프할 수 있는 기능.
- 지정된 시간만큼 앞으로 또는 뒤로 시간을 점프할 수 있는 기능. 유효한 시간 단위에는 초, 단어, 단락, 문장 및 마커가 포함되어야 합니다.
- 재생 속도를 높이거나 낮추는 기능.
- 오디오를 빨리 감기 및 빨리 되감기하는 기능, 즉 서버가 시간을 앞으로 또는 뒤로 이동할 때 오디오 스니펫이 재생되는 기능.
- 재생을 일시 중지하고 다시 시작하는 기능.
- 재생 볼륨을 높이거나 낮추는 기능.

이러한 제어는 클라이언트 사용자 인터페이스와 클라이언트의 사용자별 사용자 지정 기능을 통해 사용자가 쉽게 사용할 수 있어야 합니다. 이는 청각 장애가 있는 사용자에게 특히 중요한데, 청각 장애가 있는 사용자는 비장애 사용자에게 허용되는 것과 다른 설정 및 제어 체제를 원할 가능성이 높기 때문입니다.

---
### **4.6.  Session Parameters**

SPEECHSC 프레임워크는 언어, 음성, 발성과 같은 세션 매개변수의 사양을 지원해야 합니다.

---
### **4.7.  Speech Markers**

SPEECHSC 프레임워크는 SSML \[1\]에서 제공하는 것만큼 최소한 유연한 기능을 갖춘 음성 마커를 수용해야 합니다. 이 프레임워크는 또한 재생 중 마커에 도달했음을 보고하기 위한 효율적인 메커니즘을 제공해야 합니다.

---
## **5.  ASR Requirements**
---
### **5.1.  Requesting Automatic Speech Recognition**

SPEECHSC 프레임워크는 미디어 처리 엔터티나 애플리케이션 서버가 ASR 서버에 RTP 스트림에서 자동 음성 인식을 수행하도록 요청하고 SPEECHSC를 통해 결과를 반환할 수 있도록 허용해야 합니다.

---
### **5.2.  XML**

SPEECHSC 프레임워크는 모든 ASR 서버가 음성 인식을 위한 VoiceXML 음성 인식 문법 사양\(SRGS\)을 지원한다고 가정합니다\[2\].

---
### **5.3.  Grammar Requirements**
---
#### **5.3.1.  Grammar Specification**

SPEECHSC 프레임워크는 모든 ASR 서버가 "값으로"\(프로토콜에 내장\) 또는 "참조로"\(예: 프로토콜에 내장된 URI를 역참조하여\) 문법 사양을 허용할 수 있다고 가정합니다. 후자는 서버에 이미 알려져 있거나 그렇지 않으면 서버에 "내장된" 문법을 표시할 수 있어야 합니다. 프레임워크와 프로토콜은 또한 원래 클라이언트가 제공한 대용량 문법을 저장하고 나중에 참조로 검색할 수 있는 기능을 활용해야 합니다.

---
#### **5.3.2.  Explicit Indication of Grammar Format**

SPEECHSC 프레임워크 프로토콜은 문법이 인코딩된 문법 형식을 명시적으로 전달할 수 있어야 하며, 정의된 대로 새로운 문법 형식을 전달할 수 있도록 확장 가능해야 합니다.

---
#### **5.3.3.  Grammar Sharing**

SPEECHSC 프레임워크는 세션 간에 문법을 공유하는 것을 활용할 수 있는 서버에 대해 이를 활용해야 합니다. 이는 동적으로 로드하는 것이 비현실적인 대규모 문법이 있는 애플리케이션을 지원합니다. 예를 들어 날씨 서비스의 도시-국가 문법이 있습니다.

---
### **5.4.  Session Parameters**

SPEECHSC 프레임워크는 최소한 현재 MRCP\(Media Resource Control Protocol\)\[10\]에 정의된 모든 프로토콜 매개변수를 수용해야 합니다. 또한 세션 내에서 매개변수를 재설정하는 기능이 있어야 합니다.

---
### **5.5.  Input Capture**

SPEECHSC 프레임워크는 ASR 서버가 입력 미디어 스트림을 캡처하여 추후 ASR 엔진을 분석하고 튜닝하도록 지시하는 메서드를 지원해야 합니다.

---
## **6.  Speaker Identification and Verification Requirements**
---
### **6.1.  Requesting SI/SV**

SPEECHSC 프레임워크는 미디어 처리 엔터티가 SI/SV 서버에 RTP 스트림에 대한 스피커 식별 또는 검증을 요청하고 SPEECHSC를 통해 결과를 반환할 수 있도록 허용해야 합니다.

---
### **6.2.  Identifiers for SI/SV**

SPEECHSC 프레임워크는 각 검증 리소스에 대한 식별자를 수용하고 ID로 해당 리소스를 제어할 수 있도록 허용해야 합니다. 음성 지문 형식과 내용은 공급업체에 따라 다르기 때문입니다.

---
### **6.3.  State for Multiple Utterances**

SPEECHSC 프레임워크는 다중 발화 검증을 처리하기 위한 상태를 유지하는 SI/SV 서버와 함께 작동해야 합니다.

---
### **6.4.  Input Capture**

SPEECHSC 프레임워크는 SI/SV 엔진의 이후 분석 및 튜닝을 위해 입력 미디어 스트림을 캡처하는 방법을 지원해야 합니다. 프레임워크는 모든 서버가 이를 수행할 수 있다고 가정할 수 있습니다. 또한 프레임워크는 캡처된 스트림에 충분한 타임스탬프 컨텍스트\(예: RTP Control Protocol\(RTCP\) 패킷의 NTP 시간 범위, 이는 캡처된 입력의 RTP 타임스탬프에 해당\)가 포함되어 있어 검증이 요청된 정확한 시점을 사후에 확인할 수 있다고 가정합니다.

---
### **6.5.  SI/SV Functional Extensibility**

SPEECHSC 프레임워크는 프롬프트, 발화 검증, 재교육 등 SI/SV와 관련된 추가 기능으로 확장 가능해야 합니다.

---
## **7.  Duplexing and Parallel Operation Requirements**

대화형 음성 기반 시스템에 대한 매우 중요한 요구 사항 중 하나는 사용자가 상호 작용의 품질을 인식하는 것이 음성으로 프롬프트나 렌더링된 TTS를 중단할 수 있는 사용자의 능력에 크게 달려 있다는 것입니다. 음성 출력을 중단하거나 끼어들려면 사용자 방향에서 에너지를 감지하는 것 이상이 필요합니다. 많은 고급 시스템은 ASR 엔진을 사용하여 발화가 기침과 달리 실제 음성일 가능성이 있는지 여부를 결정하여 사용자에게 미디어를 중단합니다.

---
### **7.1.  Full Duplex Operation**

발화 감지와 재생 중단 사이의 낮은 지연 시간을 달성하기 위해 많은 구현이 말하기 기능과 ASR 기능을 결합합니다. SPEECHSC 프레임워크는 이러한 풀 듀플렉스 구현을 지원해야 합니다.

---
### **7.2.  Multiple Services in Parallel**

좋은 음성 사용자 인터페이스는 일반적으로 사용자가 작업을 얼마나 쉽게 수행할 수 있는지에 따라 달라집니다. 화자 식별 또는 검증 기술을 사용할 때 사용자 인터페이스 개선은 종종 다양한 기술의 조합에서 비롯됩니다. 동시 신원 주장 및 검증\(동일한 발화에서\), 동시 지식 및 음성 검증\(ASR과 검증을 동시에 사용\). 동일한 발화에서 ASR과 검증을 사용하는 것은 실제로 롤링 또는 동적으로 생성된 챌린지 문구\(예: "51723이라고 말하세요"\)를 지원하는 유일한 방법입니다. SPEECHSC 프레임워크는 이러한 병렬 서비스 구현을 지원해야 합니다.

---
### **7.3.  Combination of Services**

선택적으로 SPEECHSC 프레임워크가 음성 엔진의 더 복잡한 원격 조합 및 제어를 지원한다는 점이 흥미롭습니다.

- ASR, TTS 또는 스피커 인식 엔진의 입력 또는 출력에 작용할 수 있는 일련의 엔진 조합. 그런 다음 제어는 이러한 엔진을 넘어 다른 오디오 입력 및 출력 처리 및 자연어 처리를 포함할 수 있습니다.
- 엔진 간의 중간 교환 및 조정.
- 엔진 간의 흐름에 대한 원격 사양.

이러한 기능은 서비스 검색 메커니즘\(예: 엔진, 속성, 상태 검색\)으로부터 이점을 얻을 수 있습니다.

---
## **8.  Additional Considerations (Non-Normative)**

프레임워크는 SDP\(Session Description Protocol\)가 미디어 세션과 스트림을 설명하는 데 사용된다고 가정합니다. 프레임워크는 또한 미디어의 RTP 캐리지를 가정합니다. 그러나 SDP는 다른 미디어 전송 체계\(예: ATM\)를 설명하는 데 사용될 수 있으므로 필요한 요소\(예: 명시적 타임스탬프\)를 제공하는 경우 사용할 수 있습니다.

작업 그룹은 유럽 전기 통신 표준 협회\(ETSI\) 오로라 프로젝트에서 예시된 것처럼 분산 음성 인식\(DSR\) 방법을 정의하지 않습니다. 작업 그룹은 SIP 또는 SDP와 같은 다른 프로토콜에서 사용 가능한 기능을 재생성하지 않습니다.

TTS는 파일을 재생하는 것과 매우 유사합니다. RTSP 확장은 VCR 컨트롤이나 텍스트의 마커를 말해야 할 때 유망해 보입니다. VCR 컨트롤이 필요하지 않은 경우 Network Announcements \[12\]와 같은 프레임워크의 SIP는 수정 없이 직접 작동합니다.

ASR은 완전히 다른 특성을 가지고 있습니다. barge-in 지원의 경우 ASR은 중간 결과의 실시간 반환을 요구합니다. 기존 프로토콜에 대한 좋은 재사용 모델을 발견하지 않는 한, 이는 SPEECHSC의 초점이 될 가능성이 큽니다.

---
## **9.  Security Considerations**

음성 처리와 관련된 프로토콜은 보안과 프라이버시를 고려해야 합니다. 음성 기술의 많은 응용 프로그램은 재무 정보를 읽기 위한 Text-to-Speech 사용과 같은 민감한 정보를 다룹니다. 마찬가지로 자동 음성 인식의 인기 있는 용도로는 금융 거래 실행 및 쇼핑이 있습니다.

SPEECHSC 요구 사항과 교차하는 음성 처리 보안의 측면은 최소한 세 가지가 있습니다. 즉, SPEECHSC 프로토콜 자체의 보안, 프로토콜을 실행하는 서버의 구현 및 배포, 보안 기능을 제공하기 위한 기술 활용이 적절한지 확인하는 것입니다. 이러한 측면 각각은 다음 하위 섹션에서 설명합니다. 이러한 고려 사항 중 일부는 엄밀히 말해서 프로토콜 자체의 범위를 벗어나지만 프로토콜 설계 중에 신중하게 고려되고 수용되며 프로토콜 사양과 함께 제공되는 적용 가능성 설명의 일부로 언급됩니다. 개인 정보 보호 고려 사항도 논의됩니다.

---
### **9.1.  SPEECHSC Protocol Security**

SPEECHSC 프로토콜은 모든 경우에 인증, 권한 부여 및 무결성을 지원해야 하며 기밀성을 지원해야 합니다. 개인 정보 보호에 민감한 애플리케이션의 경우 프로토콜은 기밀성을 지원해야 합니다. 우리는 SPEECHSC 자체에서 프로토콜별 보안 메커니즘을 제공하는 대신, 결과 프로토콜은 포함된 프로토콜이나 프로토콜이 실행되는 전송의 보안 메커니즘을 사용할 것으로 예상합니다. 예를 들어, 제어 채널을 보호하기 위해 전송 계층 보안\(TLS\)을 사용하고 보안 실시간 전송을 사용하는 것과 같은 솔루션을 고려할 것입니다.

미디어 채널을 보호하기 위한 프로토콜\(SRTP\). 전이적 신뢰를 필요로 하는 제3자 종속성은 최소화되거나 프로토콜 설계의 인증 및 권한 부여 측면을 통해 명시적으로 처리됩니다.

---
### **9.2.  Client and Server Implementation and Deployment**

전달되는 정보의 잠재적으로 민감한 특성을 감안할 때, SPEECHSC 클라이언트와 서버는 데이터의 기밀성과 무결성 및 발화 형태로의 변환을 보장하기 위한 조치를 취해야 합니다. 이러한 일반적인 고려 사항 외에도 화자 검증 및 식별과 같은 특정 SPEECHSC 기능은 프라이버시, 기밀성 및 무결성을 유지해야 하는 음성 지문을 사용합니다. 마찬가지로 분석 및 튜닝을 위해 입력 캡처를 지원해야 하는 요구 사항은 사용자 발화가 기록되고 부적절하게 공개되거나 재생될 수 있기 때문에 개인 정보 취약성을 나타낼 수 있습니다. 구현자는 중앙 집중화된 음성 지문 데이터베이스와 이러한 음성 지문이 파생될 수 있는 기록된 자료의 악용을 방지하기 위해 주의해야 합니다. 이러한 위협을 최소화하기 위해 권장되는 구체적인 조치는 다음과 같습니다.

- 외부 공격에 대한 노출을 최소화하기 위해 데이터베이스에 대한 액세스의 종단 간 인증, 기밀성 및 무결성 보호\(TLS와 유사\)
- 내부자 위협에 대한 노출을 최소화하기 위한 읽기/쓰기 액세스 제어 및 로컬 로그인 인증과 같은 데이터베이스 보호 조치
- 특히 오프사이트 위치에 유지되는 데이터베이스 사본은 운영 데이터베이스와 동일한 보호가 필요합니다.

이 문서의 날짜에 이 데이터의 부적절한 공개는 악용 가능한 위협이 아니지만, 미래에는 그럴 가능성이 매우 큽니다. 실행 가능해질 수 있는 구체적인 취약성은 다음 하위 섹션에서 논의합니다. 음성 지문 데이터베이스를 암호화하고 적절한 권한 부여 장치를 시행하는 프로그래밍 인터페이스를 통해서만 액세스를 허용하는 것과 같은 조치를 취하는 것이 신중합니다.

---
### **9.3.  Use of SPEECHSC for Security Functions**

스피커 식별 또는 검증은 인증 기술로 직접 사용될 수 있습니다. 권한 부여 결정은 챌린지-응답 프로토콜을 통해 스피커 검증과 직접 결합되거나 액세스 제어 목록 또는 기타 ID 기반 권한 부여 메커니즘을 사용하여 스피커 식별과 간접적으로 결합될 수 있습니다. 그렇게 사용될 때,

클라이언트와 서버에 대한 프로토콜 보안 메커니즘을 사용하여 해결해야 하는 추가적인 보안 문제. 예를 들어, 스피커 검증 요청의 미디어 스트림을 조작하는 기능은 사칭이나 노이즈 주입을 통한 단순한 왜곡을 기반으로 부적절하게 액세스를 허용하거나 거부할 수 있으므로 위에서 권장한 대로 제어 및 데이터 채널을 모두 적절하게 보호하는 것이 중요합니다. 인증을 위한 SI/SV 사용과 관련된 다음 문제는 신중하게 고려해야 합니다.

1. 음성 지문이나 음성 지문을 구성하는 데 사용된 녹음된 샘플의 도난은 생체 인증 기술로서 화자 식별/확인을 사용하는 것에 대한 미래의 위협을 나타냅니다. \(오늘날에는 실행 가능하지 않은\) 그럴듯한 공격 벡터는 음성 지문 정보를 사용자의 음성을 정확하게 모방하여 음성 지문과 일치시킬 수 있는 텍스트-음성 합성 시스템에 대한 매개변수 입력으로 사용하는 것입니다. 상당히 큰 음성 샘플의 코퍼스를 은밀하게 녹음하는 것은 그리 어렵지 않기 때문에 이 공격에 대한 입력을 위한 음성 지문을 구성하는 기능은 고급 챌린지-응답 기술을 사용하더라도 음성 기반 생체 인증의 보안을 매우 취약하게 만들 것입니다. 인증을 위한 화자 확인 사용자는 이러한 미래의 취약성에 대해 이 분야의 기술 개발을 면밀히 모니터링해야 합니다\(다른 인증 기술 사용자가 비대칭 키 시스템을 깨는 방법으로 인수분해의 발전을 모니터링해야 하는 것과 마찬가지로\). 2. 다른 생체 인증 기술과 마찬가지로 음성 식별을 사용하는 것의 단점은 해지가 불가능하다는 것입니다. 일단 침해되면 생체 정보는 다른 독립 시스템에 대한 식별 및 인증에 사용될 수 있습니다. 3. 등록 절차는 프로토콜 보안 메커니즘과 일부 독립적인 신원 증명으로 보호되지 않으면 사칭에 취약할 수 있습니다. \(특정 개인과의 연관성이 아니라 등록 이후 신원 연속성만 확인하면 되는 시스템에서는 신원 증명이 필요하지 않을 수 있습니다.

SI/SV를 인증 기술로 사용하는 것에 대한 추가 논의와 장점 및 취약성에 대한 몇 가지 권장 사항은 \[15\]의 5장에서 찾을 수 있습니다.

---
## **10.  Acknowledgements**

에릭 버거는 이러한 요구 사항의 원본 버전을 작성했으며 개발 내내 적극적으로 기여했습니다. 그는 공식적인 저작을 제외한 모든 저작에서 공동 저자이며, 대신 작업 그룹 공동 의장이 문서 진행과 관련하여 충돌하지 않는 역할을 하는 것이 바람직하기 때문에 여기에서 인정합니다.

---
## **11.  References**
---
### **11.1.  Normative References**

```text
   [1]  Walker, M., Burnett, D., and A. Hunt, "Speech Synthesis Markup
        Language (SSML) Version 1.0", W3C
        REC REC-speech-synthesis-20040907, September 2004.

   [2]  McGlashan, S. and A. Hunt, "Speech Recognition Grammar
        Specification Version 1.0", W3C REC REC-speech-grammar-20040316,
        March 2004.

   [3]  Bradner, S., "Key words for use in RFCs to Indicate Requirement
        Levels", BCP 14, RFC 2119, March 1997.

   [4]  Floyd, S. and L. Daigle, "IAB Architectural and Policy
        Considerations for Open Pluggable Edge Services", RFC 3238,
        January 2002.

   [5]  Charlton, N., Gasson, M., Gybels, G., Spanner, M., and A. van
        Wijk, "User Requirements for the Session Initiation Protocol
        (SIP) in Support of Deaf, Hard of Hearing and Speech-impaired
        Individuals", RFC 3351, August 2002.
```

---
### **11.2.  Informative References**

```text
   [6]   Rosenberg, J., Schulzrinne, H., Camarillo, G., Johnston, A.,
         Peterson, J., Sparks, R., Handley, M., and E. Schooler, "SIP:
         Session Initiation Protocol", RFC 3261, June 2002.

   [7]   Andreasen, F. and B. Foster, "Media Gateway Control Protocol
         (MGCP) Version 1.0", RFC 3435, January 2003.

   [8]   Groves, C., Pantaleo, M., Ericsson, LM., Anderson, T., and T.
         Taylor, "Gateway Control Protocol Version 1", RFC 3525,
         June 2003.

   [9]   Schulzrinne, H., Rao, A., and R. Lanphier, "Real Time Streaming
         Protocol (RTSP)", RFC 2326, April 1998.

   [10]  Shanmugham, S., Monaco, P., and B. Eberman, "MRCP: Media
         Resource Control Protocol", Work in Progress.

   [11]  World Wide Web Consortium, "Voice Extensible Markup Language
         (VoiceXML) Version 2.0", W3C Working Draft , April 2002,
         <http://www.w3.org/TR/2002/WD-voicexml20-20020424/>.

   [12]  Burger, E., Ed., Van Dyke, J., and A. Spitzer, "Basic Network
         Media Services with SIP", RFC 4240, December 2005.

   [13]  Guttman, E., Perkins, C., Veizades, J., and M. Day, "Service
         Location Protocol, Version 2", RFC 2608, June 1999.

   [14]  Gulbrandsen, A., Vixie, P., and L. Esibov, "A DNS RR for
         specifying the location of services (DNS SRV)", RFC 2782,
         February 2000.

   [15]  Committee on Authentication Technologies and Their Privacy
         Implications, National Research Council, "Who Goes There?:
         Authentication Through the Lens of Privacy", Computer Science
         and Telecommunications Board (CSTB) , 2003,
         <http://www.nap.edu/catalog/10656.html/ >.
```

---
# **Author's Address**

```text
   David R. Oran
   Cisco Systems, Inc.
   7 Ladyslipper Lane
   Acton, MA
   USA

   EMail: oran@cisco.com
```

---
# **Full Copyright Statement**

저작권\(C\)인터넷학회\(2005\).

이 문서에는 BCP 78에 포함된 권리, 라이선스 및 제한 사항이 적용되며, 여기에 명시된 경우를 제외하고 작성자는 모든 권리를 보유합니다.

이 문서와 여기에 포함된 정보는 "있는 그대로" 제공되며 기여자, 그가 대표하거나 후원하는 조직\(있는 경우\), 인터넷 사회 및 인터넷 엔지니어링 태스크 포스는 모든 명시적 또는 명시적 보증을 부인합니다. 여기에 포함된 정보의 사용이 상품성이나 특정 목적에의 적합성에 대한 묵시적인 보증이나 권리를 침해하지 않는다는 보증을 포함하되 이에 국한되지 않습니다.

---
# **Intellectual Property**

IETF는 이 문서에 설명된 기술의 구현이나 사용과 관련이 있다고 주장될 수 있는 지적 재산권 또는 기타 권리의 유효성이나 범위, 그러한 권리에 따른 라이선스가 적용되거나 적용되지 않을 수 있는 범위에 대해 어떠한 입장도 취하지 않습니다. 는 가능하다; 또한 그러한 권리를 확인하기 위해 독립적인 노력을 했다는 것을 나타내지도 않습니다. RFC 문서의 권리와 관련된 절차에 대한 정보는 BCP 78 및 BCP 79에서 확인할 수 있습니다.

IETF 사무국에 제출된 IPR 공개 사본 및 사용 가능한 라이센스에 대한 보증, 또는 이 사양의 구현자 또는 사용자가 해당 독점적 권리 사용에 대한 일반 라이센스 또는 허가를 얻으려는 시도의 결과를 얻을 수 있습니다. IETF 온라인 IPR 저장소\(http://www.ietf.org/ipr\)에서.

IETF는 이 표준을 구현하는 데 필요할 수 있는 기술에 적용될 수 있는 모든 저작권, 특허, 특허 출원 또는 기타 독점권에 관심을 갖도록 관심 있는 당사자를 초대합니다. IETF\(ietf-ipr@ietf.org\)에 해당 정보를 보내주십시오.

---
# **Acknowledgement**

RFC 편집 기능을 위한 자금은 현재 Internet Society에서 제공됩니다.